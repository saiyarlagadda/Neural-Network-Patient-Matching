{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae0bb6f-5442-4803-bbe7-96ca00b8b171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67d28c2f-e136-49e6-8550-106759382a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "from faker import Faker\n",
    "from nameparser import HumanName\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from difflib import SequenceMatcher\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4152010-ae18-421b-b5b4-6b4396a224a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath=glob('/generate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213015bf-f400-47bf-95a5-441dc39e8583",
   "metadata": {},
   "source": [
    "## Check Generated data (100 Unique Patients records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a882ff90-c71c-4d14-9b03-70c7ada437ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('Generated_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e38e3a38-872b-458f-8fa7-8b739c6aeb18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>phone</th>\n",
       "      <th>address</th>\n",
       "      <th>postalZip</th>\n",
       "      <th>region</th>\n",
       "      <th>numberrange</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yvonne Phua</td>\n",
       "      <td>(833) 925-5254</td>\n",
       "      <td>Ap #437-2699 Sed Street</td>\n",
       "      <td>72001</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>06-20-2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vance Pawan</td>\n",
       "      <td>(916) 648-8254</td>\n",
       "      <td>Ap #460-3504 Lorem, Ave</td>\n",
       "      <td>24169</td>\n",
       "      <td>Montana</td>\n",
       "      <td>03-07-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lila Chande</td>\n",
       "      <td>(725) 517-5611</td>\n",
       "      <td>Ap #592-816 Vitae Rd.</td>\n",
       "      <td>85787</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>08-10-2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Signe Shaw</td>\n",
       "      <td>1-361-885-3442</td>\n",
       "      <td>Ap #247-1256 Sed, Rd.</td>\n",
       "      <td>63438</td>\n",
       "      <td>Florida</td>\n",
       "      <td>03-25-2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lucian Lambert</td>\n",
       "      <td>1-596-511-3644</td>\n",
       "      <td>401-1079 Senectus Ave</td>\n",
       "      <td>48394</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>12-08-2023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name           phone                  address  postalZip  \\\n",
       "0     Yvonne Phua  (833) 925-5254  Ap #437-2699 Sed Street      72001   \n",
       "1     Vance Pawan  (916) 648-8254  Ap #460-3504 Lorem, Ave      24169   \n",
       "2     Lila Chande  (725) 517-5611    Ap #592-816 Vitae Rd.      85787   \n",
       "3      Signe Shaw  1-361-885-3442    Ap #247-1256 Sed, Rd.      63438   \n",
       "4  Lucian Lambert  1-596-511-3644    401-1079 Senectus Ave      48394   \n",
       "\n",
       "     region numberrange  \n",
       "0      Ohio  06-20-2023  \n",
       "1   Montana  03-07-2024  \n",
       "2  Virginia  08-10-2023  \n",
       "3   Florida  03-25-2023  \n",
       "4   Georgia  12-08-2023  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ce95ef4-fa8e-47a7-bd47-09a2b3f6206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={'numberrange': 'appointmentdate'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efc3240a-a4c8-4d20-95d5-946062203530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>phone</th>\n",
       "      <th>address</th>\n",
       "      <th>postalZip</th>\n",
       "      <th>region</th>\n",
       "      <th>appointmentdate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yvonne Phua</td>\n",
       "      <td>(833) 925-5254</td>\n",
       "      <td>Ap #437-2699 Sed Street</td>\n",
       "      <td>72001</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>06-20-2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vance Pawan</td>\n",
       "      <td>(916) 648-8254</td>\n",
       "      <td>Ap #460-3504 Lorem, Ave</td>\n",
       "      <td>24169</td>\n",
       "      <td>Montana</td>\n",
       "      <td>03-07-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lila Chande</td>\n",
       "      <td>(725) 517-5611</td>\n",
       "      <td>Ap #592-816 Vitae Rd.</td>\n",
       "      <td>85787</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>08-10-2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Signe Shaw</td>\n",
       "      <td>1-361-885-3442</td>\n",
       "      <td>Ap #247-1256 Sed, Rd.</td>\n",
       "      <td>63438</td>\n",
       "      <td>Florida</td>\n",
       "      <td>03-25-2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lucian Lambert</td>\n",
       "      <td>1-596-511-3644</td>\n",
       "      <td>401-1079 Senectus Ave</td>\n",
       "      <td>48394</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>12-08-2023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name           phone                  address  postalZip  \\\n",
       "0     Yvonne Phua  (833) 925-5254  Ap #437-2699 Sed Street      72001   \n",
       "1     Vance Pawan  (916) 648-8254  Ap #460-3504 Lorem, Ave      24169   \n",
       "2     Lila Chande  (725) 517-5611    Ap #592-816 Vitae Rd.      85787   \n",
       "3      Signe Shaw  1-361-885-3442    Ap #247-1256 Sed, Rd.      63438   \n",
       "4  Lucian Lambert  1-596-511-3644    401-1079 Senectus Ave      48394   \n",
       "\n",
       "     region appointmentdate  \n",
       "0      Ohio      06-20-2023  \n",
       "1   Montana      03-07-2024  \n",
       "2  Virginia      08-10-2023  \n",
       "3   Florida      03-25-2023  \n",
       "4   Georgia      12-08-2023  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d209860-6769-4034-8c9f-caf0128029dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   name             100 non-null    object\n",
      " 1   phone            100 non-null    object\n",
      " 2   address          100 non-null    object\n",
      " 3   postalZip        100 non-null    int64 \n",
      " 4   region           100 non-null    object\n",
      " 5   appointmentdate  100 non-null    object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 4.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "420b1e6c-7268-453a-b706-52e94f9951b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          postalZip\n",
      "count    100.000000\n",
      "mean   54282.300000\n",
      "std    23495.002194\n",
      "min    12575.000000\n",
      "25%    33455.250000\n",
      "50%    55006.500000\n",
      "75%    73365.000000\n",
      "max    99811.000000\n"
     ]
    }
   ],
   "source": [
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8ef4719-130e-4257-8dd7-9361dfc0e9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name               100\n",
      "phone              100\n",
      "address            100\n",
      "postalZip           99\n",
      "region              38\n",
      "appointmentdate     91\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e47d994-c40d-4019-870a-cb89a0d3367b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name               0\n",
       "phone              0\n",
       "address            0\n",
       "postalZip          0\n",
       "region             0\n",
       "appointmentdate    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b605a915-daa1-4a4c-b8df-422bcfe86674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>postalZip</th>\n",
       "      <td>100.0</td>\n",
       "      <td>54282.3</td>\n",
       "      <td>23495.002194</td>\n",
       "      <td>12575.0</td>\n",
       "      <td>33455.25</td>\n",
       "      <td>55006.5</td>\n",
       "      <td>73365.0</td>\n",
       "      <td>99811.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count     mean           std      min       25%      50%      75%  \\\n",
       "postalZip  100.0  54282.3  23495.002194  12575.0  33455.25  55006.5  73365.0   \n",
       "\n",
       "               max  \n",
       "postalZip  99811.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1739192-fb94-4a08-988e-29328937e975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             name           phone                  address  postalZip  \\\n",
      "0     Yvonne Phua  (833) 925-5254  Ap #437-2699 Sed Street      72001   \n",
      "1     Vance Pawan  (916) 648-8254  Ap #460-3504 Lorem, Ave      24169   \n",
      "2     Lila Chande  (725) 517-5611    Ap #592-816 Vitae Rd.      85787   \n",
      "3      Signe Shaw  1-361-885-3442    Ap #247-1256 Sed, Rd.      63438   \n",
      "4  Lucian Lambert  1-596-511-3644    401-1079 Senectus Ave      48394   \n",
      "\n",
      "     region appointmentdate  \n",
      "0      Ohio      06-20-2023  \n",
      "1   Montana      03-07-2024  \n",
      "2  Virginia      08-10-2023  \n",
      "3   Florida      03-25-2023  \n",
      "4   Georgia      12-08-2023  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Sample patient data format\n",
    "# Replace this with your actual data loading code\n",
    "existing_data = data\n",
    "\n",
    "print(existing_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fba7634-1089-4279-9147-1990c7675f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_ids = random.sample(range(1000, 9999), len(existing_data))\n",
    "# existing_data['patient_id'] = unique_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b6d4e3f-be4a-41eb-8123-088c28dc0b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>phone</th>\n",
       "      <th>address</th>\n",
       "      <th>postalZip</th>\n",
       "      <th>region</th>\n",
       "      <th>appointmentdate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yvonne Phua</td>\n",
       "      <td>(833) 925-5254</td>\n",
       "      <td>Ap #437-2699 Sed Street</td>\n",
       "      <td>72001</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>06-20-2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vance Pawan</td>\n",
       "      <td>(916) 648-8254</td>\n",
       "      <td>Ap #460-3504 Lorem, Ave</td>\n",
       "      <td>24169</td>\n",
       "      <td>Montana</td>\n",
       "      <td>03-07-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lila Chande</td>\n",
       "      <td>(725) 517-5611</td>\n",
       "      <td>Ap #592-816 Vitae Rd.</td>\n",
       "      <td>85787</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>08-10-2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Signe Shaw</td>\n",
       "      <td>1-361-885-3442</td>\n",
       "      <td>Ap #247-1256 Sed, Rd.</td>\n",
       "      <td>63438</td>\n",
       "      <td>Florida</td>\n",
       "      <td>03-25-2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lucian Lambert</td>\n",
       "      <td>1-596-511-3644</td>\n",
       "      <td>401-1079 Senectus Ave</td>\n",
       "      <td>48394</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>12-08-2023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name           phone                  address  postalZip  \\\n",
       "0     Yvonne Phua  (833) 925-5254  Ap #437-2699 Sed Street      72001   \n",
       "1     Vance Pawan  (916) 648-8254  Ap #460-3504 Lorem, Ave      24169   \n",
       "2     Lila Chande  (725) 517-5611    Ap #592-816 Vitae Rd.      85787   \n",
       "3      Signe Shaw  1-361-885-3442    Ap #247-1256 Sed, Rd.      63438   \n",
       "4  Lucian Lambert  1-596-511-3644    401-1079 Senectus Ave      48394   \n",
       "\n",
       "     region appointmentdate  \n",
       "0      Ohio      06-20-2023  \n",
       "1   Montana      03-07-2024  \n",
       "2  Virginia      08-10-2023  \n",
       "3   Florida      03-25-2023  \n",
       "4   Georgia      12-08-2023  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eec7f0-55bc-45f4-b7ad-1d79fc4cf196",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(existing_data.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6a26c7-2a50-4056-b274-f18133e56206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71b2bd82-ebb1-4c10-9ae6-0cc1057a822b",
   "metadata": {},
   "source": [
    "## Generate Data (Ignored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4204e595-f360-4787-a4ba-b52978e24da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Define error simulation functions\n",
    "# def introduce_typo(text):\n",
    "#     if len(text) < 2:\n",
    "#         return text\n",
    "#     pos = random.randint(0, len(text) - 1)\n",
    "#     typo_char = random.choice(string.ascii_letters)\n",
    "#     return text[:pos] + typo_char + text[pos+1:]\n",
    "\n",
    "# def remove_char(text):\n",
    "#     if len(text) < 2:\n",
    "#         return text\n",
    "#     pos = random.randint(0, len(text) - 1)\n",
    "#     return text[:pos] + text[pos+1:]\n",
    "\n",
    "# def replace_vowel(text):\n",
    "#     vowels = 'aeiouAEIOU'\n",
    "#     if not any(c in vowels for c in text):\n",
    "#         return text\n",
    "#     new_char = random.choice(vowels)\n",
    "#     pos = random.choice([i for i, c in enumerate(text) if c in vowels])\n",
    "#     return text[:pos] + new_char + text[pos+1:]\n",
    "\n",
    "# def reverse_chars(text):\n",
    "#     if len(text) < 2:\n",
    "#         return text\n",
    "#     pos = random.randint(0, len(text) - 2)\n",
    "#     return text[:pos] + text[pos+1] + text[pos] + text[pos+2:]\n",
    "\n",
    "# def random_date_format(date):\n",
    "#     try:\n",
    "#         month, day, year = date.split('-')\n",
    "#         formats = [\n",
    "#             f'{month}-{day}', \n",
    "#             f'{day}-{month}', \n",
    "#             f'{year[2:]}-{month}-{day}',\n",
    "#             f'{month}/{day}/{year}',\n",
    "#             f'{day}/{month}/{year}',\n",
    "#             f'{month}-{day}-{year[2:]}'\n",
    "#         ]\n",
    "#         return random.choice(formats)\n",
    "#     except ValueError:\n",
    "#         # If the date is not in the expected format, return it unchanged\n",
    "#         return date\n",
    "\n",
    "# def random_alias(name, aliases):\n",
    "#     first_name, last_name = name.split(' ')\n",
    "#     return f\"{random.choice(aliases.get(first_name, [first_name]))} {last_name}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bcc8c95-6037-4af0-8545-5691cb2899a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define alias dictionary for first names\n",
    "# aliases = {\n",
    "#     'Yvonne': ['Evonne', 'Ivonne'],\n",
    "#     'Vance': ['Van', 'Vincent'],\n",
    "#     'Lila': ['Lyla', 'Lela'],\n",
    "#     'Signe': ['Signy', 'Signa'],\n",
    "#     'Lucian': ['Lucius', 'Luca'],\n",
    "#     # Add more aliases as needed\n",
    "# }\n",
    "\n",
    "# def apply_errors_to_patients(df, num_records):\n",
    "#     error_functions = [\n",
    "#         introduce_typo, remove_char, replace_vowel, reverse_chars, \n",
    "#         random_date_format, lambda x: random_alias(x, aliases)\n",
    "#     ]\n",
    "    \n",
    "#     all_records = []\n",
    "#     for _ in range(num_records):\n",
    "#         patient = df.sample().iloc[0].copy()\n",
    "#         field = random.choice(['name', 'phone', 'address', 'postalZip', 'appointmentdate'])\n",
    "        \n",
    "#         if field == 'name':\n",
    "#             error_func = random.choice(error_functions[:5] + [error_functions[5]])\n",
    "#         elif field == 'appointmentdate':\n",
    "#             error_func = error_functions[4]\n",
    "#         elif field in ['phone', 'address']:  # These are text fields\n",
    "#             error_func = random.choice(error_functions[:4])\n",
    "#         else:\n",
    "#             continue  # Skip postalZip as it is an integer and we aren't applying text errors to it\n",
    "        \n",
    "#         patient[field] = error_func(str(patient[field])) if error_func else patient[field]\n",
    "        \n",
    "#         all_records.append(patient)\n",
    "#     return pd.DataFrame(all_records)\n",
    "\n",
    "# # Generate 60,000 records with errors\n",
    "# records_with_errors = apply_errors_to_patients(existing_data, 60000)\n",
    "\n",
    "# # Save the new dataset to a CSV file\n",
    "# records_with_errors.to_csv('augmented_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "07c00e93-b6aa-4d15-aa2f-51d243592f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake = Faker()\n",
    "\n",
    "# def introduce_typo(text):\n",
    "#     if len(text) < 2:\n",
    "#         return text\n",
    "#     pos = random.randint(0, len(text) - 1)\n",
    "#     typo_char = random.choice(string.ascii_letters)\n",
    "#     return text[:pos] + typo_char + text[pos+1:]\n",
    "\n",
    "# def remove_char(text):\n",
    "#     if len(text) < 2:\n",
    "#         return text\n",
    "#     pos = random.randint(0, len(text) - 1)\n",
    "#     return text[:pos] + text[pos+1:]\n",
    "\n",
    "# def replace_vowel(text):\n",
    "#     vowels = 'aeiouAEIOU'\n",
    "#     if not any(c in vowels for c in text):\n",
    "#         return text\n",
    "#     new_char = random.choice(vowels)\n",
    "#     pos = random.choice([i for i, c in enumerate(text) if c in vowels])\n",
    "#     return text[:pos] + new_char + text[pos+1:]\n",
    "\n",
    "# def reverse_chars(text):\n",
    "#     if len(text) < 2:\n",
    "#         return text\n",
    "#     pos = random.randint(0, len(text) - 2)\n",
    "#     return text[:pos] + text[pos+1] + text[pos] + text[pos+2:]\n",
    "\n",
    "# def generate_alias(name):\n",
    "#     # Generate a random first name using Faker and keep the last name\n",
    "#     first_name, last_name = name.split(' ')\n",
    "#     new_first_name = fake.first_name()\n",
    "#     return f\"{new_first_name} {last_name}\"\n",
    "\n",
    "# # List of error functions to be applied to names\n",
    "# name_error_functions = [introduce_typo, remove_char, replace_vowel, reverse_chars, generate_alias]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cc9509b9-460a-4102-9f77-04c1bfee1ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def apply_errors_to_patients(df, num_records):\n",
    "#     all_records = []\n",
    "#     for _ in range(num_records):\n",
    "#         patient = df.sample().iloc[0].copy()\n",
    "        \n",
    "#         # Only apply errors to the 'name' field\n",
    "#         error_func = random.choice(name_error_functions)\n",
    "#         patient['name'] = error_func(patient['name'])\n",
    "        \n",
    "#         all_records.append(patient)\n",
    "#     return pd.DataFrame(all_records)\n",
    "\n",
    "# # Generate 60,000 records with errors\n",
    "# records_with_errors = apply_errors_to_patients(existing_data, 60000)\n",
    "\n",
    "# # Save the new dataset to a CSV file\n",
    "# records_with_errors.to_csv('Gen_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "89958a35-6189-4844-b1e2-83630e7dc8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_name(full_name):\n",
    "    name = HumanName(full_name)\n",
    "    return {\n",
    "        'first': name.first,\n",
    "        'middle': name.middle,\n",
    "        'last': name.last\n",
    "    }\n",
    "\n",
    "def format_name(parsed_name):\n",
    "    middle = f\" {parsed_name['middle']}\" if parsed_name['middle'] else \"\"\n",
    "    return f\"{parsed_name['last']}, {parsed_name['first']}{middle}\"\n",
    "\n",
    "def parse_address(address):\n",
    "    parts = address.split(',')\n",
    "    address1 = parts[0].strip() if len(parts) > 0 else ''\n",
    "    address2 = parts[1].strip() if len(parts) > 1 else ''\n",
    "    address3 = parts[2].strip() if len(parts) > 2 else ''\n",
    "    return address1, address2, address3\n",
    "\n",
    "def format_address(address1, address2, address3, city, state, zip_code):\n",
    "    formatted_address = address1\n",
    "    if address2:\n",
    "        formatted_address += f\"{address2}\"\n",
    "    if address3:\n",
    "        formatted_address += f\"{address3}\"\n",
    "    formatted_address += f\"{city}, {zip_code}\"\n",
    "    return formatted_address.strip()\n",
    "\n",
    "def introduce_typo(text):\n",
    "    if len(text) < 2:\n",
    "        return text\n",
    "    pos = random.randint(0, len(text) - 1)\n",
    "    typo_char = random.choice(string.ascii_letters)\n",
    "    return text[:pos] + typo_char + text[pos+1:]\n",
    "\n",
    "def remove_char(text):\n",
    "    if len(text) < 2:\n",
    "        return text\n",
    "    pos = random.randint(0, len(text) - 1)\n",
    "    return text[:pos] + text[pos+1:]\n",
    "\n",
    "def replace_vowel(text):\n",
    "    vowels = 'aeiouAEIOU'\n",
    "    if not any(c in vowels for c in text):\n",
    "        return text\n",
    "    new_char = random.choice(vowels)\n",
    "    pos = random.choice([i for i, c in enumerate(text) if c in vowels])\n",
    "    return text[:pos] + new_char + text[pos+1:]\n",
    "\n",
    "def reverse_chars(text):\n",
    "    if len(text) < 2:\n",
    "        return text\n",
    "    pos = random.randint(0, len(text) - 2)\n",
    "    return text[:pos] + text[pos+1] + text[pos] + text[pos+2:]\n",
    "\n",
    "# List of error functions to be applied to names\n",
    "name_error_functions = [introduce_typo, remove_char, replace_vowel, reverse_chars]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "236a6684-d922-49bc-b0b0-d07626acc67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             name           phone                               address  \\\n",
      "0    Phua, Yvonnu  (833) 925-5254    Ap #437-2699 Sed StreetOhio, 72001   \n",
      "1     Pawan, Vnce  (916) 648-8254   Ap #460-3504 LoremAveMontana, 24169   \n",
      "2    ChandA, Lila  (725) 517-5611  Ap #592-816 Vitae Rd.Virginia, 85787   \n",
      "3     Shaw, Signe  1-361-885-3442     Ap #247-1256 SedRd.Florida, 63438   \n",
      "4  Lambert, Lucin  1-596-511-3644   401-1079 Senectus AveGeorgia, 48394   \n",
      "\n",
      "   postalZip    region appointmentdate  patient_id  \n",
      "0      72001      Ohio      06-20-2023        3070  \n",
      "1      24169   Montana      03-07-2024        6404  \n",
      "2      85787  Virginia      08-10-2023        1134  \n",
      "3      63438   Florida      03-25-2023        8685  \n",
      "4      48394   Georgia      12-08-2023        7823  \n"
     ]
    }
   ],
   "source": [
    "def apply_errors_and_format(df):\n",
    "    all_records = []\n",
    "    for _, row in df.iterrows():\n",
    "        patient = row.copy()\n",
    "        \n",
    "        # Apply errors to the 'name' field\n",
    "        error_func = random.choice(name_error_functions)\n",
    "        patient['name'] = error_func(patient['name'])\n",
    "        \n",
    "        # Parse and format name\n",
    "        parsed_name = parse_name(patient['name'])\n",
    "        patient['name'] = format_name(parsed_name)\n",
    "        \n",
    "        # Parse and format address\n",
    "        address1, address2, address3 = parse_address(patient['address'])\n",
    "        patient['address'] = format_address(address1, address2, address3, patient['region'], '', patient['postalZip'])\n",
    "        \n",
    "        all_records.append(patient)\n",
    "    return pd.DataFrame(all_records)\n",
    "\n",
    "# Apply errors and format fields on the existing data\n",
    "updated_records = apply_errors_and_format(existing_data)\n",
    "\n",
    "# Save the updated dataset to a CSV file\n",
    "updated_records.to_csv('updated_augmented_data.csv', index=False)\n",
    "\n",
    "# Check the updated records\n",
    "print(updated_records.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948d260f-a123-452d-9dbe-072708be1f4a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Check 60k Data (Ignored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "debb2721-cbc6-4f6c-a9e8-d970e81d5bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_data=pd.read_csv('updated_augmented_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6f4d5fb3-a775-45c3-8b74-c1170f0d3d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>phone</th>\n",
       "      <th>address</th>\n",
       "      <th>postalZip</th>\n",
       "      <th>region</th>\n",
       "      <th>appointmentdate</th>\n",
       "      <th>patient_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Phua, Yvonnu</td>\n",
       "      <td>(833) 925-5254</td>\n",
       "      <td>Ap #437-2699 Sed StreetOhio, 72001</td>\n",
       "      <td>72001</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>06-20-2023</td>\n",
       "      <td>3070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pawan, Vnce</td>\n",
       "      <td>(916) 648-8254</td>\n",
       "      <td>Ap #460-3504 LoremAveMontana, 24169</td>\n",
       "      <td>24169</td>\n",
       "      <td>Montana</td>\n",
       "      <td>03-07-2024</td>\n",
       "      <td>6404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ChandA, Lila</td>\n",
       "      <td>(725) 517-5611</td>\n",
       "      <td>Ap #592-816 Vitae Rd.Virginia, 85787</td>\n",
       "      <td>85787</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>08-10-2023</td>\n",
       "      <td>1134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shaw, Signe</td>\n",
       "      <td>1-361-885-3442</td>\n",
       "      <td>Ap #247-1256 SedRd.Florida, 63438</td>\n",
       "      <td>63438</td>\n",
       "      <td>Florida</td>\n",
       "      <td>03-25-2023</td>\n",
       "      <td>8685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lambert, Lucin</td>\n",
       "      <td>1-596-511-3644</td>\n",
       "      <td>401-1079 Senectus AveGeorgia, 48394</td>\n",
       "      <td>48394</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>12-08-2023</td>\n",
       "      <td>7823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name           phone                               address  \\\n",
       "0    Phua, Yvonnu  (833) 925-5254    Ap #437-2699 Sed StreetOhio, 72001   \n",
       "1     Pawan, Vnce  (916) 648-8254   Ap #460-3504 LoremAveMontana, 24169   \n",
       "2    ChandA, Lila  (725) 517-5611  Ap #592-816 Vitae Rd.Virginia, 85787   \n",
       "3     Shaw, Signe  1-361-885-3442     Ap #247-1256 SedRd.Florida, 63438   \n",
       "4  Lambert, Lucin  1-596-511-3644   401-1079 Senectus AveGeorgia, 48394   \n",
       "\n",
       "   postalZip    region appointmentdate  patient_id  \n",
       "0      72001      Ohio      06-20-2023        3070  \n",
       "1      24169   Montana      03-07-2024        6404  \n",
       "2      85787  Virginia      08-10-2023        1134  \n",
       "3      63438   Florida      03-25-2023        8685  \n",
       "4      48394   Georgia      12-08-2023        7823  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bd7aa7c8-03fc-4c3d-9628-eda9ba8ebe2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name               100\n",
       "phone              100\n",
       "address            100\n",
       "postalZip           99\n",
       "region              38\n",
       "appointmentdate     91\n",
       "patient_id         100\n",
       "dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_data.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7687dd4e-b907-43e1-b0e3-868dc4f4b838",
   "metadata": {},
   "source": [
    "## TEST_Success (Generated 60K Records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "263f0b22-d8f6-4926-b9df-77280a41dfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nameparser import HumanName\n",
    "import string\n",
    "\n",
    "def parse_name(full_name):\n",
    "    name = HumanName(full_name)\n",
    "    return {\n",
    "        'first': name.first,\n",
    "        'middle': name.middle,\n",
    "        'last': name.last\n",
    "    }\n",
    "\n",
    "def format_name(parsed_name):\n",
    "    middle = f\" {parsed_name['middle']}\" if parsed_name['middle'] else \"\"\n",
    "    return f\"{parsed_name['last']}, {parsed_name['first']}{middle}\"\n",
    "\n",
    "def parse_address(address):\n",
    "    parts = address.split(',')\n",
    "    address1 = parts[0].strip() if len(parts) > 0 else ''\n",
    "    address2 = parts[1].strip() if len(parts) > 1 else ''\n",
    "    address3 = parts[2].strip() if len(parts) > 2 else ''\n",
    "    return address1, address2, address3\n",
    "\n",
    "def format_address(address1, address2, address3, city, zip_code):\n",
    "    formatted_address = f\"{address1}\"\n",
    "    if address2:\n",
    "        formatted_address += f\", {address2}\"\n",
    "    if address3:\n",
    "        formatted_address += f\", {address3}\"\n",
    "    formatted_address += f\"\\n{city}, {zip_code}\"\n",
    "    return formatted_address.strip()\n",
    "\n",
    "def introduce_typo(text):\n",
    "    if len(text) < 2:\n",
    "        return text\n",
    "    pos = random.randint(0, len(text) - 1)\n",
    "    typo_char = random.choice(string.ascii_letters)\n",
    "    return text[:pos] + typo_char + text[pos+1:]\n",
    "\n",
    "def remove_char(text):\n",
    "    if len(text) < 2:\n",
    "        return text\n",
    "    pos = random.randint(0, len(text) - 1)\n",
    "    return text[:pos] + text[pos+1:]\n",
    "\n",
    "def replace_vowel(text):\n",
    "    vowels = 'aeiouAEIOU'\n",
    "    if not any(c in vowels for c in text):\n",
    "        return text\n",
    "    new_char = random.choice(vowels)\n",
    "    pos = random.choice([i for i, c in enumerate(text) if c in vowels])\n",
    "    return text[:pos] + new_char + text[pos+1:]\n",
    "\n",
    "def reverse_chars(text):\n",
    "    if len(text) < 2:\n",
    "        return text\n",
    "    pos = random.randint(0, len(text) - 2)\n",
    "    return text[:pos] + text[pos+1] + text[pos] + text[pos+2:]\n",
    "\n",
    "# List of error functions to be applied to names\n",
    "name_error_functions = [introduce_typo, remove_char, replace_vowel, reverse_chars]\n",
    "\n",
    "def random_date_format(date):\n",
    "    month, day, year = date.split('-')\n",
    "    formats = [\n",
    "        f'{month}-{day}',           # MM-DD\n",
    "        f'{day}-{month}',           # DD-MM\n",
    "        f'{month}-{day}-{year}',    # MM-DD-YYYY\n",
    "        f'{day}-{month}-{year}',    # DD-MM-YYYY\n",
    "        f'{month}-{day}-{year[2:]}',# MM-DD-YY\n",
    "        f'{day}-{month}-{year[2:]}',# DD-MM-YY\n",
    "    ]\n",
    "    return random.choice(formats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff351421-505b-4ff7-9052-7436a83f31eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          name           phone                  address  postalZip region  \\\n",
      "0  Yvonne Phua  (833) 925-5254  Ap #437-2699 Sed Street      72001   Ohio   \n",
      "0  YvonnE Phua  (833) 925-5254  Ap #437-2699 Sed Street      72001   Ohio   \n",
      "0   Yvonne hua  (833) 925-5254  Ap #437-2699 Sed Street      72001   Ohio   \n",
      "0   Yvnne Phua  (833) 925-5254  Ap #437-2699 Sed Street      72001   Ohio   \n",
      "0  Yvonne Phau  (833) 925-5254  Ap #437-2699 Sed Street      72001   Ohio   \n",
      "\n",
      "  appointmentdate  \n",
      "0      06-20-2023  \n",
      "0      06-20-2023  \n",
      "0      20-06-2023  \n",
      "0      20-06-2023  \n",
      "0           20-06  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 60000 entries, 0 to 99\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   name             60000 non-null  object\n",
      " 1   phone            60000 non-null  object\n",
      " 2   address          60000 non-null  object\n",
      " 3   postalZip        60000 non-null  int64 \n",
      " 4   region           60000 non-null  object\n",
      " 5   appointmentdate  60000 non-null  object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 3.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def apply_errors_to_name(name):\n",
    "    error_func = random.choice(name_error_functions)\n",
    "    return error_func(name)\n",
    "\n",
    "def generate_records_with_errors(df, num_records):\n",
    "    records_per_patient = num_records // len(df)\n",
    "    remaining_records = num_records % len(df)\n",
    "\n",
    "    all_records = []\n",
    "    for _, row in df.iterrows():\n",
    "        patient_records = [row.copy() for _ in range(records_per_patient)]\n",
    "        \n",
    "        # Apply errors to the name field for duplicates only\n",
    "        for i in range(1, len(patient_records)):\n",
    "            patient_records[i]['name'] = apply_errors_to_name(patient_records[i]['name'])\n",
    "        \n",
    "        # Apply errors to the appointmentdate field for all records\n",
    "        for record in patient_records:\n",
    "            record['appointmentdate'] = random_date_format(record['appointmentdate'])\n",
    "        \n",
    "        all_records.extend(patient_records)\n",
    "\n",
    "    # Add remaining records if any\n",
    "    if remaining_records > 0:\n",
    "        remaining_records = [row.copy() for _, row in df.sample(n=remaining_records).iterrows()]\n",
    "        for i in range(1, len(remaining_records)):\n",
    "            remaining_records[i]['name'] = apply_errors_to_name(remaining_records[i]['name'])\n",
    "        for record in remaining_records:\n",
    "            record['appointmentdate'] = random_date_format(record['appointmentdate'])\n",
    "        all_records.extend(remaining_records)\n",
    "    \n",
    "    return pd.DataFrame(all_records)\n",
    "\n",
    "# Generate 60,000 records with errors in names for duplicates and appointment dates\n",
    "records_with_errors = generate_records_with_errors(existing_data, 60000)\n",
    "\n",
    "# Save the new dataset to a CSV file\n",
    "records_with_errors.to_csv('augmented_data_with_errors.csv', index=False)\n",
    "\n",
    "# Check the updated records\n",
    "print(records_with_errors.head())\n",
    "print(records_with_errors.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e66c7b99-b2f9-482a-bdf6-4faa72853aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('augmented_data_with_errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abc76702-97b1-41c6-b259-91efdc5c71e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name               18242\n",
       "phone                100\n",
       "address              100\n",
       "postalZip             99\n",
       "region                38\n",
       "appointmentdate      515\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6026c30b-1a54-489b-a3c9-3f9a4860a405",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Test (Failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b64b3075-1185-4564-a06b-00b452314635",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_data=pd.read_csv('Gen_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c18de6ea-fa9a-4aa3-935e-506680c939cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>phone</th>\n",
       "      <th>address</th>\n",
       "      <th>postalZip</th>\n",
       "      <th>region</th>\n",
       "      <th>appointmentdate</th>\n",
       "      <th>patient_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kam Sehgal</td>\n",
       "      <td>(494) 262-1158</td>\n",
       "      <td>Ap #504-7735 Erat Road</td>\n",
       "      <td>81337</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>05-18-2023</td>\n",
       "      <td>9907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dennis Sgra</td>\n",
       "      <td>(417) 774-9342</td>\n",
       "      <td>4030 Orci, Av.</td>\n",
       "      <td>66399</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>04-26-2024</td>\n",
       "      <td>4760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brett Thomson</td>\n",
       "      <td>1-618-565-9763</td>\n",
       "      <td>888-1043 Tristique Avenue</td>\n",
       "      <td>86070</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>04-13-2023</td>\n",
       "      <td>7676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dennis Sraa</td>\n",
       "      <td>(417) 774-9342</td>\n",
       "      <td>4030 Orci, Av.</td>\n",
       "      <td>66399</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>04-26-2024</td>\n",
       "      <td>4760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Paki Thmson</td>\n",
       "      <td>1-618-565-9763</td>\n",
       "      <td>888-1043 Tristique Avenue</td>\n",
       "      <td>86070</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>04-13-2023</td>\n",
       "      <td>7676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name           phone                    address  postalZip  \\\n",
       "0     Kam Sehgal  (494) 262-1158     Ap #504-7735 Erat Road      81337   \n",
       "1    Dennis Sgra  (417) 774-9342             4030 Orci, Av.      66399   \n",
       "2  Brett Thomson  1-618-565-9763  888-1043 Tristique Avenue      86070   \n",
       "3    Dennis Sraa  (417) 774-9342             4030 Orci, Av.      66399   \n",
       "4    Paki Thmson  1-618-565-9763  888-1043 Tristique Avenue      86070   \n",
       "\n",
       "      region appointmentdate  patient_id  \n",
       "0  Louisiana      05-18-2023        9907  \n",
       "1     Oregon      04-26-2024        4760  \n",
       "2     Alaska      04-13-2023        7676  \n",
       "3     Oregon      04-26-2024        4760  \n",
       "4     Alaska      04-13-2023        7676  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "242a669a-9029-4a11-85aa-7df25012b56a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a53cc149-510f-4ade-8979-b17e2733e439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name               25338\n",
       "phone                100\n",
       "address              100\n",
       "postalZip             99\n",
       "region                38\n",
       "appointmentdate       91\n",
       "patient_id           100\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_data.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406f6e3d-f6da-47cb-837c-ac54e6144a1e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Test Generate Data (Failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e94e8ab-966a-408b-9e07-1c2e9b4b5e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test=pd.read_csv('augmented_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05403649-110c-4380-a5f7-9e732cc9bc52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>phone</th>\n",
       "      <th>address</th>\n",
       "      <th>postalZip</th>\n",
       "      <th>region</th>\n",
       "      <th>appointmentdate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zachery Legault</td>\n",
       "      <td>1-824-551-4145</td>\n",
       "      <td>7748 Arcu Rd.</td>\n",
       "      <td>52158</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>09-25-2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Melinda Hall</td>\n",
       "      <td>1-429-456-4436</td>\n",
       "      <td>Ap #88O-4291 Sed St.</td>\n",
       "      <td>67319</td>\n",
       "      <td>Nebraska</td>\n",
       "      <td>12-19-2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Luke Pok</td>\n",
       "      <td>(385) 710-2162</td>\n",
       "      <td>9836 aFucibus St.</td>\n",
       "      <td>72781</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>04-24-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Signe Shaw</td>\n",
       "      <td>1-361-885-3442</td>\n",
       "      <td>Ap #247-1256 SUd, Rd.</td>\n",
       "      <td>63438</td>\n",
       "      <td>Florida</td>\n",
       "      <td>03-25-2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dai Tucker</td>\n",
       "      <td>(357) 346-3055</td>\n",
       "      <td>Ap #193-6352 Lorem St.</td>\n",
       "      <td>22745</td>\n",
       "      <td>Nebraska</td>\n",
       "      <td>23-04-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name           phone                 address  postalZip  \\\n",
       "0  Zachery Legault  1-824-551-4145           7748 Arcu Rd.      52158   \n",
       "1     Melinda Hall  1-429-456-4436    Ap #88O-4291 Sed St.      67319   \n",
       "2         Luke Pok  (385) 710-2162       9836 aFucibus St.      72781   \n",
       "3       Signe Shaw  1-361-885-3442   Ap #247-1256 SUd, Rd.      63438   \n",
       "4       Dai Tucker  (357) 346-3055  Ap #193-6352 Lorem St.      22745   \n",
       "\n",
       "      region appointmentdate  \n",
       "0   Kentucky      09-25-2023  \n",
       "1   Nebraska      12-19-2023  \n",
       "2  Tennessee      04-24-2024  \n",
       "3    Florida      03-25-2023  \n",
       "4   Nebraska        23-04-24  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4aadafbd-c763-499f-8c5e-c48c27561a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48166, 6)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "471b9db6-4814-4c85-b5c4-ccc4f4fbb3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48166 entries, 0 to 48165\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   name             48166 non-null  object\n",
      " 1   phone            48166 non-null  object\n",
      " 2   address          48166 non-null  object\n",
      " 3   postalZip        48166 non-null  int64 \n",
      " 4   region           48166 non-null  object\n",
      " 5   appointmentdate  48166 non-null  object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "data_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b7dd611-c250-4ec5-bea1-0bbcc47d2d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name               5272\n",
       "phone              5291\n",
       "address            7801\n",
       "postalZip            99\n",
       "region               38\n",
       "appointmentdate     613\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff119459-8227-480a-8253-313a7482122e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Test Generated Data (Ignored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa2cb995-2744-423c-9b67-100338942c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of                     name           phone                      address  \\\n",
       "0             Kam Sehgal  (494) 262-1158       Ap #504-7735 Erat Road   \n",
       "1            Dennis Sgra  (417) 774-9342               4030 Orci, Av.   \n",
       "2          Brett Thomson  1-618-565-9763    888-1043 Tristique Avenue   \n",
       "3            Dennis Sraa  (417) 774-9342               4030 Orci, Av.   \n",
       "4            Paki Thmson  1-618-565-9763    888-1043 Tristique Avenue   \n",
       "...                  ...             ...                          ...   \n",
       "59995    Lauren Mitchell  (468) 482-3138  435-7980 Scelerisque Avenue   \n",
       "59996        Julian Leng  (928) 720-6784            482-3671 Sed Road   \n",
       "59997  JosephEne Colling  (900) 555-1559              700-7438 In Rd.   \n",
       "59998        Luke Murphy  1-341-313-8267         Ap #714-5830 Et, Rd.   \n",
       "59999          Jin SmUth  1-314-638-6883   P.O. Box 755, 4875 Per Rd.   \n",
       "\n",
       "       postalZip     region appointmentdate  patient_id  \n",
       "0          81337  Louisiana      05-18-2023        9907  \n",
       "1          66399     Oregon      04-26-2024        4760  \n",
       "2          86070     Alaska      04-13-2023        7676  \n",
       "3          66399     Oregon      04-26-2024        4760  \n",
       "4          86070     Alaska      04-13-2023        7676  \n",
       "...          ...        ...             ...         ...  \n",
       "59995      45244   Colorado      01-09-2023        7472  \n",
       "59996      35994    Montana      04-30-2024        2983  \n",
       "59997      86353   Oklahoma      01-10-2023        9728  \n",
       "59998      53214   Illinois      08-30-2023        5168  \n",
       "59999      63392    Wyoming      03-06-2024        1787  \n",
       "\n",
       "[60000 rows x 7 columns]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_data.info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51028f5a-3a48-4265-8f0e-ed114999b97c",
   "metadata": {},
   "source": [
    "# Integrating Alias_analisis work (Failed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecfeddc-cf51-4470-ad1b-e3f2726a34aa",
   "metadata": {},
   "source": [
    "## Convert GloVe Embeddings to Pickle Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00834fce-d45e-486a-a454-06631274a90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# def save_glove_to_pickle(glove_file_path, pickle_file_path):\n",
    "#     embeddings_dict = {}\n",
    "#     with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             values = line.split()\n",
    "#             word = values[0]\n",
    "#             vector = list(map(float, values[1:]))\n",
    "#             embeddings_dict[word] = vector\n",
    "#     with open(pickle_file_path, 'wb') as f:\n",
    "#         pickle.dump(embeddings_dict, f)\n",
    "\n",
    "# glove_file_path = '/Users/saiyarlagadda/Desktop/Neural-Network-Patient-Matching/glove.6B/glove.6B.100d.txt'\n",
    "# pickle_file_path = '/Users/saiyarlagadda/Desktop/Neural-Network-Patient-Matching/my_project/alias_analysis/pickle-dicts/glove-6B-100d.pkl'\n",
    "# save_glove_to_pickle(glove_file_path, pickle_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fbce31e-4de0-4ef6-81f4-8056f54532f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/saiyarlagadda/Desktop/Neural-Network-Patient-Matching/glove.6B/glove.6B.50d.txt and converting to dictionary...\n",
      "Processed 0 lines...\n",
      "Processed 100000 lines...\n",
      "Processed 200000 lines...\n",
      "Processed 300000 lines...\n",
      "GloVe embeddings successfully saved to /Users/saiyarlagadda/Desktop/Neural-Network-Patient-Matching/my_project/alias_analysis/pickle-dicts/glove.6B.50d.pkl\n",
      "Reading /Users/saiyarlagadda/Desktop/Neural-Network-Patient-Matching/glove.6B/glove.6B.100d.txt and converting to dictionary...\n",
      "Processed 0 lines...\n",
      "Processed 100000 lines...\n",
      "Processed 200000 lines...\n",
      "Processed 300000 lines...\n",
      "GloVe embeddings successfully saved to /Users/saiyarlagadda/Desktop/Neural-Network-Patient-Matching/my_project/alias_analysis/pickle-dicts/glove.6B.100d.pkl\n",
      "Reading /Users/saiyarlagadda/Desktop/Neural-Network-Patient-Matching/glove.6B/glove.6B.200d.txt and converting to dictionary...\n",
      "Processed 0 lines...\n",
      "Processed 100000 lines...\n",
      "Processed 200000 lines...\n",
      "Processed 300000 lines...\n",
      "GloVe embeddings successfully saved to /Users/saiyarlagadda/Desktop/Neural-Network-Patient-Matching/my_project/alias_analysis/pickle-dicts/glove.6B.200d.pkl\n",
      "Reading /Users/saiyarlagadda/Desktop/Neural-Network-Patient-Matching/glove.6B/glove.6B.300d.txt and converting to dictionary...\n",
      "Processed 0 lines...\n",
      "Processed 100000 lines...\n",
      "Processed 200000 lines...\n",
      "Processed 300000 lines...\n",
      "GloVe embeddings successfully saved to /Users/saiyarlagadda/Desktop/Neural-Network-Patient-Matching/my_project/alias_analysis/pickle-dicts/glove.6B.300d.pkl\n"
     ]
    }
   ],
   "source": [
    "def save_glove_to_pickle(glove_file_path, pickle_file_path):\n",
    "    \"\"\"Convert a GloVe text file to a pickle file.\"\"\"\n",
    "    embeddings_dict = {}\n",
    "    try:\n",
    "        with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
    "            print(f\"Reading {glove_file_path} and converting to dictionary...\")\n",
    "            for i, line in enumerate(f):\n",
    "                if i % 100000 == 0:\n",
    "                    print(f\"Processed {i} lines...\")\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = list(map(float, values[1:]))\n",
    "                embeddings_dict[word] = vector\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading GloVe file {glove_file_path}: {e}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        with open(pickle_file_path, 'wb') as f:\n",
    "            pickle.dump(embeddings_dict, f)\n",
    "        print(f\"GloVe embeddings successfully saved to {pickle_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to pickle file {pickle_file_path}: {e}\")\n",
    "\n",
    "# List of GloVe file paths\n",
    "glove_files = [\n",
    "    # '/Users/saiyarlagadda/Desktop/Neural-Network-Patient-Matching/glove.6B/glove.42B.300d.txt',\n",
    "    '/Users/saiyarlagadda/Desktop/Neural-Network-Patient-Matching/glove.6B/glove.6B.50d.txt',\n",
    "    '/Users/saiyarlagadda/Desktop/Neural-Network-Patient-Matching/glove.6B/glove.6B.100d.txt',\n",
    "    '/Users/saiyarlagadda/Desktop/Neural-Network-Patient-Matching/glove.6B/glove.6B.200d.txt',\n",
    "    '/Users/saiyarlagadda/Desktop/Neural-Network-Patient-Matching/glove.6B/glove.6B.300d.txt'\n",
    "]\n",
    "\n",
    "# Directory for pickle files\n",
    "pickle_dir = '/Users/saiyarlagadda/Desktop/Neural-Network-Patient-Matching/my_project/alias_analysis/pickle-dicts'\n",
    "\n",
    "# Ensure the pickle directory exists\n",
    "os.makedirs(pickle_dir, exist_ok=True)\n",
    "\n",
    "# Process each GloVe file\n",
    "for glove_file in glove_files:\n",
    "    glove_name = os.path.basename(glove_file).replace('.txt', '.pkl')\n",
    "    pickle_file_path = os.path.join(pickle_dir, glove_name)\n",
    "    save_glove_to_pickle(glove_file, pickle_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061b18bb-3bbf-4651-a277-f5584410205e",
   "metadata": {},
   "source": [
    "## Load the GloVe Embeddings from Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0eb99619-6e03-4f01-ac0a-252e9ad9f02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_glove_embeddings_from_pickle(pickle_file_path):\n",
    "#     with open(pickle_file_path, 'rb') as f:\n",
    "#         embeddings_dict = pickle.load(f)\n",
    "#     return embeddings_dict\n",
    "\n",
    "# # Path to your pickle file\n",
    "# pickle_file_path = '/Users/saiyarlagadda/Desktop/Neural-Network-Patient-Matching/my_project/alias_analysis/pickle-dicts/glove-6B-100d.pkl'\n",
    "# glove_embeddings = load_glove_embeddings_from_pickle(pickle_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0fb955bd-6bf3-4691-97fd-9c93a235bb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embeddings from /Users/saiyarlagadda/Desktop/Neural-Network-Patient-Matching/my_project/alias_analysis/pickle-dicts/glove.6B.50d.pkl\n",
      "Loaded embeddings from /Users/saiyarlagadda/Desktop/Neural-Network-Patient-Matching/my_project/alias_analysis/pickle-dicts/glove.6B.300d.pkl\n",
      "Loaded embeddings from /Users/saiyarlagadda/Desktop/Neural-Network-Patient-Matching/my_project/alias_analysis/pickle-dicts/glove.6B.100d.pkl\n",
      "File does not exist: /Users/saiyarlagadda/Desktop/Neural-Network-Patient-Matching/my_project/alias_analysis/pickle-dicts/glove.42B.300d.pkl\n",
      "Loaded embeddings from /Users/saiyarlagadda/Desktop/Neural-Network-Patient-Matching/my_project/alias_analysis/pickle-dicts/glove.6B.200d.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def load_glove_embeddings_from_pickle(pickle_file_path):\n",
    "    \"\"\"Load GloVe embeddings from a pickle file.\"\"\"\n",
    "    try:\n",
    "        with open(pickle_file_path, 'rb') as f:\n",
    "            embeddings_dict = pickle.load(f)\n",
    "        print(f\"Loaded embeddings from {pickle_file_path}\")\n",
    "        return embeddings_dict\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {pickle_file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading GloVe embeddings from {pickle_file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Directory containing pickle files\n",
    "pickle_dir = '/Users/saiyarlagadda/Desktop/Neural-Network-Patient-Matching/my_project/alias_analysis/pickle-dicts'\n",
    "\n",
    "# List files in the directory\n",
    "# print(\"Files in directory:\")\n",
    "# for file in os.listdir(pickle_dir):\n",
    "#     print(file)\n",
    "\n",
    "# List of pickle files to load\n",
    "# Verify files before including them in this list\n",
    "pickle_files = [\n",
    "'glove.6B.50d.pkl',\n",
    "'glove.6B.300d.pkl',\n",
    "'glove.6B.100d.pkl',\n",
    "'glove.42B.300d.pkl',\n",
    "'glove.6B.200d.pkl'\n",
    "]\n",
    "\n",
    "# Dictionary to store loaded embeddings\n",
    "glove_embeddings_dict = {}\n",
    "\n",
    "# Load embeddings from each pickle file\n",
    "for pickle_file in pickle_files:\n",
    "    pickle_file_path = os.path.join(pickle_dir, pickle_file)\n",
    "    if os.path.exists(pickle_file_path):\n",
    "        embeddings = load_glove_embeddings_from_pickle(pickle_file_path)\n",
    "        if embeddings is not None:\n",
    "            glove_embeddings_dict[pickle_file] = embeddings\n",
    "    else:\n",
    "        print(f\"File does not exist: {pickle_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "894df962-06c0-402c-b3a2-70a833ec5ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.spatial import distance\n",
    "\n",
    "# def find_closest_embeddings(embedding, embeddings_dict, top_n=5):\n",
    "#     return sorted(embeddings_dict.keys(), key=lambda word: distance.euclidean(embeddings_dict[word], embedding))[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8cfad914-a5f1-4adf-a4a7-8b1b16b40114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def find_closest_embeddings(embedding, embeddings_dict, top_n=5):\n",
    "    \"\"\"Find the top_n closest embeddings in embeddings_dict to the given embedding.\"\"\"\n",
    "    return sorted(\n",
    "        embeddings_dict.keys(),\n",
    "        key=lambda word: distance.euclidean(embeddings_dict[word], embedding)\n",
    "    )[:top_n]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a05ab8-39dd-4826-85b8-cc4100e4920e",
   "metadata": {},
   "source": [
    "## Processing your Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f94ac8ff-6e8a-4699-a004-8b32efc23879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('my_project/alias_analysis/data/60k_records.csv')\n",
    "# Extract the names\n",
    "names = data['name'].str.lower().unique()\n",
    "# tried to use the 100unique patients data to find the embedding match.\n",
    "# reference purpose only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cc377a93-fa5b-4468-9ba0-11ce352365d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yvonne phua' 'vance pawan' 'lila chande' 'signe shaw' 'lucian lambert'\n",
      " 'aspen bernard' 'ayanna williams' 'melinda hall' 'armand proulx'\n",
      " 'paki thomson' 'martha harrison' 'kasper tong' 'caesar olson' 'charde hu'\n",
      " 'thor ko' 'zoe zimmermann' 'dieter wolff' 'amanda dupuis' 'macy cole'\n",
      " 'ulysses chan' 'elton wong' 'fiona jin' 'giacomo aggarwal' 'cora james'\n",
      " 'jakeem perlmann' 'chaim cook' 'yardley zhong' 'martena foster'\n",
      " 'josephine colling' 'mackenzie veena' 'amena persaud' 'kai png'\n",
      " 'tanner scott' 'constance sanders' 'julian leong' 'leilani rajagopal'\n",
      " 'cairo watson' 'barrett rojas' 'perry lalit' 'samson shu' 'raya burns'\n",
      " 'kirsten boyd' 'unity nelson' 'mackensie sharma' 'adrian snyder'\n",
      " 'raya bhat' 'karleigh carter' 'kim sehgal' 'harper morgan'\n",
      " 'salvador kraus' 'hadassah bell' 'riley leclerc' 'jackson lee'\n",
      " 'andrew cheng' 'shana sehgal' 'rahim schumann' 'jolie white'\n",
      " 'zachery legault' 'ralph scholz' 'hayden perry' 'guy kelly'\n",
      " 'hayley ranga' 'odette lao' 'wesley chauhan' 'indira sen' 'myles murphy'\n",
      " 'demetria leng' 'salvador matthews' 'stacy martin' 'georgia hamilton'\n",
      " 'neve bennett' 'hedley huang' 'luke pok' 'dolan charan' 'adena warren'\n",
      " 'iona punj' 'florence mitchell' 'elton lopez' 'jin smith' 'xander turner'\n",
      " 'kelly romann' 'dennis sara' 'liberty seet' 'uma nigam'\n",
      " 'zephania elliott' 'maggie perez' 'mckenzie allen' 'abbot lehmann'\n",
      " 'indigo mitchell' 'cameron morgan' 'molly guzman' 'stuart sethi'\n",
      " 'abraham hunter' 'leigh zheng' 'brian xia' 'lance parker' 'dai tucker'\n",
      " 'tamekah joshi' 'cyrus lata' 'anthony mills']\n"
     ]
    }
   ],
   "source": [
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e67d2bf-7822-49d7-86d5-716ad6c5f263",
   "metadata": {},
   "source": [
    "## Find Alias for Each Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "767c333f-6c57-487c-b7ab-ce3ff3123e4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# aliases_dict = {}\n",
    "# for name in names:\n",
    "#     try:\n",
    "#         embedding = glove_embeddings[name]\n",
    "#         closest_names = find_closest_embeddings(embedding, glove_embeddings)\n",
    "#         aliases_dict[name] = closest_names\n",
    "#     except KeyError:\n",
    "#         print(f\"Word '{name}' not found in GloVe embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4fa98ce6-43b0-4b94-bace-497a5747e262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'yvonne phua' not found in any GloVe embeddings dictionaries.\n",
      "Word 'vance pawan' not found in any GloVe embeddings dictionaries.\n",
      "Word 'lila chande' not found in any GloVe embeddings dictionaries.\n",
      "Word 'signe shaw' not found in any GloVe embeddings dictionaries.\n",
      "Word 'lucian lambert' not found in any GloVe embeddings dictionaries.\n",
      "Word 'aspen bernard' not found in any GloVe embeddings dictionaries.\n",
      "Word 'ayanna williams' not found in any GloVe embeddings dictionaries.\n",
      "Word 'melinda hall' not found in any GloVe embeddings dictionaries.\n",
      "Word 'armand proulx' not found in any GloVe embeddings dictionaries.\n",
      "Word 'paki thomson' not found in any GloVe embeddings dictionaries.\n",
      "Word 'martha harrison' not found in any GloVe embeddings dictionaries.\n",
      "Word 'kasper tong' not found in any GloVe embeddings dictionaries.\n",
      "Word 'caesar olson' not found in any GloVe embeddings dictionaries.\n",
      "Word 'charde hu' not found in any GloVe embeddings dictionaries.\n",
      "Word 'thor ko' not found in any GloVe embeddings dictionaries.\n",
      "Word 'zoe zimmermann' not found in any GloVe embeddings dictionaries.\n",
      "Word 'dieter wolff' not found in any GloVe embeddings dictionaries.\n",
      "Word 'amanda dupuis' not found in any GloVe embeddings dictionaries.\n",
      "Word 'macy cole' not found in any GloVe embeddings dictionaries.\n",
      "Word 'ulysses chan' not found in any GloVe embeddings dictionaries.\n",
      "Word 'elton wong' not found in any GloVe embeddings dictionaries.\n",
      "Word 'fiona jin' not found in any GloVe embeddings dictionaries.\n",
      "Word 'giacomo aggarwal' not found in any GloVe embeddings dictionaries.\n",
      "Word 'cora james' not found in any GloVe embeddings dictionaries.\n",
      "Word 'jakeem perlmann' not found in any GloVe embeddings dictionaries.\n",
      "Word 'chaim cook' not found in any GloVe embeddings dictionaries.\n",
      "Word 'yardley zhong' not found in any GloVe embeddings dictionaries.\n",
      "Word 'martena foster' not found in any GloVe embeddings dictionaries.\n",
      "Word 'josephine colling' not found in any GloVe embeddings dictionaries.\n",
      "Word 'mackenzie veena' not found in any GloVe embeddings dictionaries.\n",
      "Word 'amena persaud' not found in any GloVe embeddings dictionaries.\n",
      "Word 'kai png' not found in any GloVe embeddings dictionaries.\n",
      "Word 'tanner scott' not found in any GloVe embeddings dictionaries.\n",
      "Word 'constance sanders' not found in any GloVe embeddings dictionaries.\n",
      "Word 'julian leong' not found in any GloVe embeddings dictionaries.\n",
      "Word 'leilani rajagopal' not found in any GloVe embeddings dictionaries.\n",
      "Word 'cairo watson' not found in any GloVe embeddings dictionaries.\n",
      "Word 'barrett rojas' not found in any GloVe embeddings dictionaries.\n",
      "Word 'perry lalit' not found in any GloVe embeddings dictionaries.\n",
      "Word 'samson shu' not found in any GloVe embeddings dictionaries.\n",
      "Word 'raya burns' not found in any GloVe embeddings dictionaries.\n",
      "Word 'kirsten boyd' not found in any GloVe embeddings dictionaries.\n",
      "Word 'unity nelson' not found in any GloVe embeddings dictionaries.\n",
      "Word 'mackensie sharma' not found in any GloVe embeddings dictionaries.\n",
      "Word 'adrian snyder' not found in any GloVe embeddings dictionaries.\n",
      "Word 'raya bhat' not found in any GloVe embeddings dictionaries.\n",
      "Word 'karleigh carter' not found in any GloVe embeddings dictionaries.\n",
      "Word 'kim sehgal' not found in any GloVe embeddings dictionaries.\n",
      "Word 'harper morgan' not found in any GloVe embeddings dictionaries.\n",
      "Word 'salvador kraus' not found in any GloVe embeddings dictionaries.\n",
      "Word 'hadassah bell' not found in any GloVe embeddings dictionaries.\n",
      "Word 'riley leclerc' not found in any GloVe embeddings dictionaries.\n",
      "Word 'jackson lee' not found in any GloVe embeddings dictionaries.\n",
      "Word 'andrew cheng' not found in any GloVe embeddings dictionaries.\n",
      "Word 'shana sehgal' not found in any GloVe embeddings dictionaries.\n",
      "Word 'rahim schumann' not found in any GloVe embeddings dictionaries.\n",
      "Word 'jolie white' not found in any GloVe embeddings dictionaries.\n",
      "Word 'zachery legault' not found in any GloVe embeddings dictionaries.\n",
      "Word 'ralph scholz' not found in any GloVe embeddings dictionaries.\n",
      "Word 'hayden perry' not found in any GloVe embeddings dictionaries.\n",
      "Word 'guy kelly' not found in any GloVe embeddings dictionaries.\n",
      "Word 'hayley ranga' not found in any GloVe embeddings dictionaries.\n",
      "Word 'odette lao' not found in any GloVe embeddings dictionaries.\n",
      "Word 'wesley chauhan' not found in any GloVe embeddings dictionaries.\n",
      "Word 'indira sen' not found in any GloVe embeddings dictionaries.\n",
      "Word 'myles murphy' not found in any GloVe embeddings dictionaries.\n",
      "Word 'demetria leng' not found in any GloVe embeddings dictionaries.\n",
      "Word 'salvador matthews' not found in any GloVe embeddings dictionaries.\n",
      "Word 'stacy martin' not found in any GloVe embeddings dictionaries.\n",
      "Word 'georgia hamilton' not found in any GloVe embeddings dictionaries.\n",
      "Word 'neve bennett' not found in any GloVe embeddings dictionaries.\n",
      "Word 'hedley huang' not found in any GloVe embeddings dictionaries.\n",
      "Word 'luke pok' not found in any GloVe embeddings dictionaries.\n",
      "Word 'dolan charan' not found in any GloVe embeddings dictionaries.\n",
      "Word 'adena warren' not found in any GloVe embeddings dictionaries.\n",
      "Word 'iona punj' not found in any GloVe embeddings dictionaries.\n",
      "Word 'florence mitchell' not found in any GloVe embeddings dictionaries.\n",
      "Word 'elton lopez' not found in any GloVe embeddings dictionaries.\n",
      "Word 'jin smith' not found in any GloVe embeddings dictionaries.\n",
      "Word 'xander turner' not found in any GloVe embeddings dictionaries.\n",
      "Word 'kelly romann' not found in any GloVe embeddings dictionaries.\n",
      "Word 'dennis sara' not found in any GloVe embeddings dictionaries.\n",
      "Word 'liberty seet' not found in any GloVe embeddings dictionaries.\n",
      "Word 'uma nigam' not found in any GloVe embeddings dictionaries.\n",
      "Word 'zephania elliott' not found in any GloVe embeddings dictionaries.\n",
      "Word 'maggie perez' not found in any GloVe embeddings dictionaries.\n",
      "Word 'mckenzie allen' not found in any GloVe embeddings dictionaries.\n",
      "Word 'abbot lehmann' not found in any GloVe embeddings dictionaries.\n",
      "Word 'indigo mitchell' not found in any GloVe embeddings dictionaries.\n",
      "Word 'cameron morgan' not found in any GloVe embeddings dictionaries.\n",
      "Word 'molly guzman' not found in any GloVe embeddings dictionaries.\n",
      "Word 'stuart sethi' not found in any GloVe embeddings dictionaries.\n",
      "Word 'abraham hunter' not found in any GloVe embeddings dictionaries.\n",
      "Word 'leigh zheng' not found in any GloVe embeddings dictionaries.\n",
      "Word 'brian xia' not found in any GloVe embeddings dictionaries.\n",
      "Word 'lance parker' not found in any GloVe embeddings dictionaries.\n",
      "Word 'dai tucker' not found in any GloVe embeddings dictionaries.\n",
      "Word 'tamekah joshi' not found in any GloVe embeddings dictionaries.\n",
      "Word 'cyrus lata' not found in any GloVe embeddings dictionaries.\n",
      "Word 'anthony mills' not found in any GloVe embeddings dictionaries.\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "# from scipy.spatial import distance\n",
    "\n",
    "# aliases_dict = {}\n",
    "\n",
    "# # Assuming 'names' is a list of unique names from your data\n",
    "# for name in names:\n",
    "#     closest_names = []\n",
    "#     found_embedding = False\n",
    "    \n",
    "#     # Iterate over each loaded GloVe embedding dictionary\n",
    "#     for key, embeddings_dict in glove_embeddings_dict.items():\n",
    "#         try:\n",
    "#             embedding = embeddings_dict[name]\n",
    "#             closest = find_closest_embeddings(embedding, embeddings_dict)\n",
    "#             closest_names.extend(closest)\n",
    "#             found_embedding = True\n",
    "#         except KeyError:\n",
    "#             continue\n",
    "    \n",
    "#     if found_embedding:\n",
    "#         aliases_dict[name] = closest_names[:5]  # Store top 5 closest names\n",
    "#     else:\n",
    "#         print(f\"Word '{name}' not found in any GloVe embeddings dictionaries.\")\n",
    "\n",
    "# # Print or use aliases_dict as needed\n",
    "# print(aliases_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e8f666e-faa7-4ec5-b231-1a0de4c908f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_composite_embedding(name, embeddings_dict):\n",
    "    \"\"\"Get a composite embedding for a multi-word name by averaging word embeddings.\"\"\"\n",
    "    words = name.lower().split()  # Split name into words and convert to lowercase\n",
    "    valid_embeddings = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in embeddings_dict:\n",
    "            valid_embeddings.append(embeddings_dict[word])\n",
    "    \n",
    "    if not valid_embeddings:\n",
    "        raise KeyError(f\"No valid embeddings found for words in '{name}'\")\n",
    "    \n",
    "    return np.mean(valid_embeddings, axis=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8700aebf-1e1f-4fb8-b6c9-c3b6bcd6f5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_embeddings(embedding, embeddings_dict, top_n=5):\n",
    "    return sorted(embeddings_dict.keys(), key=lambda word: distance.euclidean(embeddings_dict[word], embedding))[:top_n]\n",
    "\n",
    "aliases_dict = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7fdb45-a546-41f4-a98a-54315e18cfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'yvonnecphua' not found in any GloVe embeddings dictionaries.\n",
      "Word 'yvonn ephua' not found in any GloVe embeddings dictionaries.\n",
      "Word 'yvonnezphua' not found in any GloVe embeddings dictionaries.\n",
      "Word 'yvonnephua' not found in any GloVe embeddings dictionaries.\n",
      "Word 'yvonneaphua' not found in any GloVe embeddings dictionaries.\n",
      "Word 'yvonneophua' not found in any GloVe embeddings dictionaries.\n",
      "Word 'yvonnejphua' not found in any GloVe embeddings dictionaries.\n",
      "Word 'yvonnebphua' not found in any GloVe embeddings dictionaries.\n",
      "Word 'yvonneuphua' not found in any GloVe embeddings dictionaries.\n",
      "Word 'yvonnekphua' not found in any GloVe embeddings dictionaries.\n",
      "Word 'yvonnexphua' not found in any GloVe embeddings dictionaries.\n",
      "Word 'yvonnehphua' not found in any GloVe embeddings dictionaries.\n",
      "Word 'vancespawan' not found in any GloVe embeddings dictionaries.\n",
      "Word 'vancecpawan' not found in any GloVe embeddings dictionaries.\n",
      "Word 'vancepawan' not found in any GloVe embeddings dictionaries.\n",
      "Word 'vancetpawan' not found in any GloVe embeddings dictionaries.\n",
      "Word 'vancekpawan' not found in any GloVe embeddings dictionaries.\n",
      "Word 'vancevpawan' not found in any GloVe embeddings dictionaries.\n",
      "Word 'vanceapawan' not found in any GloVe embeddings dictionaries.\n",
      "Word 'vancejpawan' not found in any GloVe embeddings dictionaries.\n",
      "Word 'vancedpawan' not found in any GloVe embeddings dictionaries.\n",
      "Word 'vancegpawan' not found in any GloVe embeddings dictionaries.\n",
      "Word 'vancehpawan' not found in any GloVe embeddings dictionaries.\n",
      "Word 'vancezpawan' not found in any GloVe embeddings dictionaries.\n",
      "Word 'vancefpawan' not found in any GloVe embeddings dictionaries.\n",
      "Word 'lilachande' not found in any GloVe embeddings dictionaries.\n",
      "Word 'lilaqchande' not found in any GloVe embeddings dictionaries.\n",
      "Word 'lilavchande' not found in any GloVe embeddings dictionaries.\n",
      "Word 'lilarchande' not found in any GloVe embeddings dictionaries.\n",
      "Word 'lilaychande' not found in any GloVe embeddings dictionaries.\n",
      "Word 'lilatchande' not found in any GloVe embeddings dictionaries.\n",
      "Word 'liladchande' not found in any GloVe embeddings dictionaries.\n",
      "Word 'lilazchande' not found in any GloVe embeddings dictionaries.\n",
      "Word 'signeeshaw' not found in any GloVe embeddings dictionaries.\n",
      "Word 'signeshaw' not found in any GloVe embeddings dictionaries.\n",
      "Word 'signejshaw' not found in any GloVe embeddings dictionaries.\n",
      "Word 'signexshaw' not found in any GloVe embeddings dictionaries.\n",
      "Word 'signewshaw' not found in any GloVe embeddings dictionaries.\n",
      "Word 'signesshaw' not found in any GloVe embeddings dictionaries.\n",
      "Word 'signepshaw' not found in any GloVe embeddings dictionaries.\n",
      "Word 'signehshaw' not found in any GloVe embeddings dictionaries.\n",
      "Word 'signemshaw' not found in any GloVe embeddings dictionaries.\n",
      "Word 'signezshaw' not found in any GloVe embeddings dictionaries.\n",
      "Word 'signeyshaw' not found in any GloVe embeddings dictionaries.\n",
      "Word 'signershaw' not found in any GloVe embeddings dictionaries.\n",
      "Word 'lucianrlambert' not found in any GloVe embeddings dictionaries.\n",
      "Word 'lucianl ambert' not found in any GloVe embeddings dictionaries.\n",
      "Word 'lucianlambert' not found in any GloVe embeddings dictionaries.\n",
      "Word 'lucianalambert' not found in any GloVe embeddings dictionaries.\n",
      "Word 'lucianflambert' not found in any GloVe embeddings dictionaries.\n",
      "Word 'lucianglambert' not found in any GloVe embeddings dictionaries.\n",
      "Word 'lucianilambert' not found in any GloVe embeddings dictionaries.\n",
      "Word 'lucianolambert' not found in any GloVe embeddings dictionaries.\n",
      "Word 'lucianylambert' not found in any GloVe embeddings dictionaries.\n",
      "Word 'lucianslambert' not found in any GloVe embeddings dictionaries.\n",
      "Word 'lucianklambert' not found in any GloVe embeddings dictionaries.\n",
      "Word 'aspenb ernard' not found in any GloVe embeddings dictionaries.\n",
      "Word 'aspenubernard' not found in any GloVe embeddings dictionaries.\n",
      "Word 'aspenbernard' not found in any GloVe embeddings dictionaries.\n",
      "Word 'aspenvbernard' not found in any GloVe embeddings dictionaries.\n",
      "Word 'aspenobernard' not found in any GloVe embeddings dictionaries.\n",
      "Word 'aspenrbernard' not found in any GloVe embeddings dictionaries.\n",
      "Word 'aspenlbernard' not found in any GloVe embeddings dictionaries.\n",
      "Word 'aspenbbernard' not found in any GloVe embeddings dictionaries.\n",
      "Word 'aspenxbernard' not found in any GloVe embeddings dictionaries.\n",
      "Word 'aspenjbernard' not found in any GloVe embeddings dictionaries.\n",
      "Word 'aspenqbernard' not found in any GloVe embeddings dictionaries.\n",
      "Word 'aspenabernard' not found in any GloVe embeddings dictionaries.\n",
      "Word 'ayannawilliams' not found in any GloVe embeddings dictionaries.\n",
      "Word 'ayann awilliams' not found in any GloVe embeddings dictionaries.\n",
      "Word 'ayannapwilliams' not found in any GloVe embeddings dictionaries.\n",
      "Word 'ayannamwilliams' not found in any GloVe embeddings dictionaries.\n",
      "Word 'ayannaw illiams' not found in any GloVe embeddings dictionaries.\n",
      "Word 'ayannauwilliams' not found in any GloVe embeddings dictionaries.\n",
      "Word 'ayannaowilliams' not found in any GloVe embeddings dictionaries.\n",
      "Word 'ayannafwilliams' not found in any GloVe embeddings dictionaries.\n",
      "Word 'ayannatwilliams' not found in any GloVe embeddings dictionaries.\n",
      "Word 'ayannadwilliams' not found in any GloVe embeddings dictionaries.\n",
      "Word 'ayannajwilliams' not found in any GloVe embeddings dictionaries.\n",
      "Word 'melindadhall' not found in any GloVe embeddings dictionaries.\n",
      "Word 'melindahall' not found in any GloVe embeddings dictionaries.\n",
      "Word 'melind ahall' not found in any GloVe embeddings dictionaries.\n",
      "Word 'melindajhall' not found in any GloVe embeddings dictionaries.\n",
      "Word 'melindathall' not found in any GloVe embeddings dictionaries.\n",
      "Word 'melindaehall' not found in any GloVe embeddings dictionaries.\n",
      "Word 'melindayhall' not found in any GloVe embeddings dictionaries.\n",
      "Word 'melindamhall' not found in any GloVe embeddings dictionaries.\n",
      "Word 'melindaqhall' not found in any GloVe embeddings dictionaries.\n",
      "Word 'melindanhall' not found in any GloVe embeddings dictionaries.\n",
      "Word 'melindashall' not found in any GloVe embeddings dictionaries.\n",
      "Word 'melindachall' not found in any GloVe embeddings dictionaries.\n",
      "Word 'melindarhall' not found in any GloVe embeddings dictionaries.\n",
      "Word 'melindafhall' not found in any GloVe embeddings dictionaries.\n",
      "Word 'armanduproulx' not found in any GloVe embeddings dictionaries.\n",
      "Word 'armandp roulx' not found in any GloVe embeddings dictionaries.\n",
      "Word 'armandproulx' not found in any GloVe embeddings dictionaries.\n",
      "Word 'armandrproulx' not found in any GloVe embeddings dictionaries.\n",
      "Word 'armandvproulx' not found in any GloVe embeddings dictionaries.\n",
      "Word 'armandeproulx' not found in any GloVe embeddings dictionaries.\n",
      "Word 'armandmproulx' not found in any GloVe embeddings dictionaries.\n",
      "Word 'armandsproulx' not found in any GloVe embeddings dictionaries.\n",
      "Word 'armandcproulx' not found in any GloVe embeddings dictionaries.\n",
      "Word 'armandbproulx' not found in any GloVe embeddings dictionaries.\n",
      "Word 'armandoproulx' not found in any GloVe embeddings dictionaries.\n",
      "Word 'armandqproulx' not found in any GloVe embeddings dictionaries.\n",
      "Word 'armandhproulx' not found in any GloVe embeddings dictionaries.\n",
      "Word 'armandlproulx' not found in any GloVe embeddings dictionaries.\n",
      "Word 'armandiproulx' not found in any GloVe embeddings dictionaries.\n",
      "Word 'armandzproulx' not found in any GloVe embeddings dictionaries.\n",
      "Word 'pakisthomson' not found in any GloVe embeddings dictionaries.\n",
      "Word 'pakit homson' not found in any GloVe embeddings dictionaries.\n",
      "Word 'pakilthomson' not found in any GloVe embeddings dictionaries.\n",
      "Word 'pakithomson' not found in any GloVe embeddings dictionaries.\n",
      "Word 'pakidthomson' not found in any GloVe embeddings dictionaries.\n",
      "Word 'pakihthomson' not found in any GloVe embeddings dictionaries.\n",
      "Word 'pakizthomson' not found in any GloVe embeddings dictionaries.\n",
      "Word 'pakiythomson' not found in any GloVe embeddings dictionaries.\n",
      "Word 'pakiqthomson' not found in any GloVe embeddings dictionaries.\n",
      "Word 'pakiuthomson' not found in any GloVe embeddings dictionaries.\n",
      "Word 'pakifthomson' not found in any GloVe embeddings dictionaries.\n",
      "Word 'pakikthomson' not found in any GloVe embeddings dictionaries.\n",
      "Word 'pakiethomson' not found in any GloVe embeddings dictionaries.\n",
      "Word 'pakibthomson' not found in any GloVe embeddings dictionaries.\n",
      "Word 'pakijthomson' not found in any GloVe embeddings dictionaries.\n",
      "Word 'marthamharrison' not found in any GloVe embeddings dictionaries.\n",
      "Word 'marthaharrison' not found in any GloVe embeddings dictionaries.\n",
      "Word 'marthahharrison' not found in any GloVe embeddings dictionaries.\n",
      "Word 'marthaeharrison' not found in any GloVe embeddings dictionaries.\n",
      "Word 'marthaaharrison' not found in any GloVe embeddings dictionaries.\n",
      "Word 'marthah arrison' not found in any GloVe embeddings dictionaries.\n",
      "Word 'marthagharrison' not found in any GloVe embeddings dictionaries.\n",
      "Word 'marthalharrison' not found in any GloVe embeddings dictionaries.\n",
      "Word 'martharharrison' not found in any GloVe embeddings dictionaries.\n",
      "Word 'marthawharrison' not found in any GloVe embeddings dictionaries.\n",
      "Word 'marthacharrison' not found in any GloVe embeddings dictionaries.\n",
      "Word 'kaspervtong' not found in any GloVe embeddings dictionaries.\n",
      "Word 'kaspe rtong' not found in any GloVe embeddings dictionaries.\n",
      "Word 'kaspergtong' not found in any GloVe embeddings dictionaries.\n",
      "Word 'kasperqtong' not found in any GloVe embeddings dictionaries.\n",
      "Word 'kaspertong' not found in any GloVe embeddings dictionaries.\n",
      "Word 'kasperktong' not found in any GloVe embeddings dictionaries.\n",
      "Word 'kasperbtong' not found in any GloVe embeddings dictionaries.\n",
      "Word 'kasperxtong' not found in any GloVe embeddings dictionaries.\n",
      "Word 'kasperrtong' not found in any GloVe embeddings dictionaries.\n",
      "Word 'kasperctong' not found in any GloVe embeddings dictionaries.\n",
      "Word 'kasperatong' not found in any GloVe embeddings dictionaries.\n",
      "Word 'kasperutong' not found in any GloVe embeddings dictionaries.\n",
      "Word 'kasperptong' not found in any GloVe embeddings dictionaries.\n",
      "Word 'kasperztong' not found in any GloVe embeddings dictionaries.\n",
      "Word 'kasperjtong' not found in any GloVe embeddings dictionaries.\n",
      "Word 'kasperftong' not found in any GloVe embeddings dictionaries.\n",
      "Word 'kasperotong' not found in any GloVe embeddings dictionaries.\n",
      "Word 'kasperitong' not found in any GloVe embeddings dictionaries.\n",
      "Word 'caesa rolson' not found in any GloVe embeddings dictionaries.\n",
      "Word 'caesarfolson' not found in any GloVe embeddings dictionaries.\n",
      "Word 'caesaro lson' not found in any GloVe embeddings dictionaries.\n",
      "Word 'caesarsolson' not found in any GloVe embeddings dictionaries.\n",
      "Word 'caesarqolson' not found in any GloVe embeddings dictionaries.\n",
      "Word 'caesarolson' not found in any GloVe embeddings dictionaries.\n",
      "Word 'caesaroolson' not found in any GloVe embeddings dictionaries.\n",
      "Word 'caesardolson' not found in any GloVe embeddings dictionaries.\n",
      "Word 'caesarkolson' not found in any GloVe embeddings dictionaries.\n",
      "Word 'caesareolson' not found in any GloVe embeddings dictionaries.\n",
      "Word 'caesarbolson' not found in any GloVe embeddings dictionaries.\n",
      "Word 'caesarmolson' not found in any GloVe embeddings dictionaries.\n",
      "Word 'caesarnolson' not found in any GloVe embeddings dictionaries.\n",
      "Word 'chardehu' not found in any GloVe embeddings dictionaries.\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each name in names\n",
    "for name in names:\n",
    "    closest_names = []\n",
    "    found_embedding = False\n",
    "    \n",
    "    # Iterate over each loaded GloVe embedding dictionary\n",
    "    for key, embeddings_dict in glove_embeddings_dict.items():\n",
    "        try:\n",
    "            # Get composite embedding for the name\n",
    "            embedding = get_composite_embedding(name, embeddings_dict)\n",
    "            # Find closest embeddings\n",
    "            closest = find_closest_embeddings(embedding, embeddings_dict)\n",
    "            closest_names.extend(closest)\n",
    "            found_embedding = True\n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    if found_embedding:\n",
    "        # Remove duplicates and take the top 5 closest names\n",
    "        aliases_dict[name] = list(dict.fromkeys(closest_names))[:5]\n",
    "    else:\n",
    "        print(f\"Word '{name}' not found in any GloVe embeddings dictionaries.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceb90a8-0590-4f42-a870-a9a1dca991a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print or use aliases_dict as needed\n",
    "print(aliases_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e80b458-2a13-4bd2-bffc-c4e5b3f3028c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Save and integrate Aliasis in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "399bdf04-528f-4d66-b791-bb9b044c242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aliases_df = pd.DataFrame.from_dict(aliases_dict, orient='index')\n",
    "aliases_df.to_csv('/Users/saiyarlagadda/Desktop/Neural-Network-Patient-Matching/aliases.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71107e1a-9ada-4360-bfa5-ead7f87b9ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_primary_alias(name, aliases_dict):\n",
    "    return aliases_dict.get(name, [name])[0]\n",
    "\n",
    "df['alias'] = df['name'].str.lower().apply(lambda x: get_primary_alias(x, aliases_dict))\n",
    "df.to_csv('/Users/saiyarlagadda/Desktop/Neural-Network-Patient-Matching/60k_dataset_with_aliases.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36c9af0-f309-4d76-8f15-fc60be5fbd4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee3ab57a-3765-4f6f-8b76-ad076f338c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          name           phone                  address  postalZip region  \\\n",
      "0  Yvonne Phua  (833) 925-5254  Ap #437-2699 Sed Street      72001   Ohio   \n",
      "1  YvonnE Phua  (833) 925-5254  Ap #437-2699 Sed Street      72001   Ohio   \n",
      "2   Yvonne hua  (833) 925-5254  Ap #437-2699 Sed Street      72001   Ohio   \n",
      "3   Yvnne Phua  (833) 925-5254  Ap #437-2699 Sed Street      72001   Ohio   \n",
      "4  Yvonne Phau  (833) 925-5254  Ap #437-2699 Sed Street      72001   Ohio   \n",
      "\n",
      "  appointmentdate        alias  \n",
      "0      06-20-2023  yvonne phua  \n",
      "1      06-20-2023  yvonne phua  \n",
      "2      20-06-2023   yvonne hua  \n",
      "3      20-06-2023   yvnne phua  \n",
      "4           20-06  yvonne phau  \n",
      "          name        alias\n",
      "0  Yvonne Phua  yvonne phua\n",
      "1  YvonnE Phua  yvonne phua\n",
      "2   Yvonne hua   yvonne hua\n",
      "3   Yvnne Phua   yvnne phua\n",
      "4  Yvonne Phau  yvonne phau\n",
      "5  Yvonfe Phua  yvonfe phua\n",
      "6  zvonne Phua  zvonne phua\n",
      "7  Yvonne Phue  yvonne phue\n",
      "8  Yvonnu Phua  yvonnu phua\n",
      "9   Yvnne Phua   yvnne phua\n",
      "Name: Luk ePok, Primary Alias: luk epok\n",
      "Name: Zphania Elliott, Primary Alias: zphania elliott\n",
      "Name: Zephania ulliott, Primary Alias: zephania ulliott\n",
      "Name: Julian Loeng, Primary Alias: julian loeng\n",
      "Name: Salvador Krais, Primary Alias: salvador krais\n",
      "Name: Georgia Hamilton, Primary Alias: georgia hamilton\n",
      "Name: Salvador Kruas, Primary Alias: salvador kruas\n",
      "Name: Maggi ePerez, Primary Alias: maggi eperez\n",
      "Name: Jolce White, Primary Alias: jolce white\n",
      "Name: EltIn Lopez, Primary Alias: eltin lopez\n",
      "alias\n",
      "neve bennett         72\n",
      "zoe zimmermann       67\n",
      "ulysses chan         62\n",
      "barrett rojas        60\n",
      "abbot lehmann        59\n",
      "maggie perez         59\n",
      "ayanna williams      59\n",
      "chaim cook           58\n",
      "indigo mitchell      58\n",
      "yvonne phua          58\n",
      "jakeem perlmann      57\n",
      "tanner scott         57\n",
      "kelly romann        57\n",
      "hadassah bell        56\n",
      "melinda hall         56\n",
      "anthony mills        55\n",
      "jackson lee          54\n",
      "guy kelly            52\n",
      "josephine colling    51\n",
      "dieter wolff         50\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the updated dataset\n",
    "df_with_aliases = pd.read_csv('/Users/saiyarlagadda/Desktop/Neural-Network-Patient-Matching/60k_dataset_with_aliases.csv')\n",
    "\n",
    "# Display the first few rows to verify alias column\n",
    "print(df_with_aliases.head())\n",
    "\n",
    "# Check the alias column for a sample of names\n",
    "print(df_with_aliases[['name', 'alias']].head(10))\n",
    "\n",
    "# Verify a sample of aliases\n",
    "sample_names = df_with_aliases['name'].sample(10).tolist()\n",
    "for name in sample_names:\n",
    "    primary_alias = df_with_aliases.loc[df_with_aliases['name'] == name, 'alias'].values[0]\n",
    "    print(f\"Name: {name}, Primary Alias: {primary_alias}\")\n",
    "\n",
    "# Analyze the distribution of aliases\n",
    "alias_counts = df_with_aliases['alias'].value_counts()\n",
    "print(alias_counts.head(20))\n",
    "\n",
    "# Save and backup your work\n",
    "df_with_aliases.to_csv('/Users/saiyarlagadda/Desktop/Neural-Network-Patient-Matching/60k_dataset_with_aliases.csv', index=False)\n",
    "aliases_df.to_csv('/Users/saiyarlagadda/Desktop/Neural-Network-Patient-Matching/aliases_backup.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1748d7c3-aafc-4606-bd00-811abcfde823",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Test_June (Failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4050475-3204-46d4-a647-bc912e633fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the GloVe embeddings\n",
    "with open('/Users/saiyarlagadda/Desktop/Neural-Network-Patient-Matching/my_project/alias_analysis/pickle-dicts/glove-6B-100d.pkl', 'rb') as f:\n",
    "    embeddings_dict = pickle.load(f)\n",
    "\n",
    "# Function to get vector for a word, with fallback to average vector\n",
    "def get_word_vector(word, embeddings_dict):\n",
    "    word = word.lower()\n",
    "    if word in embeddings_dict:\n",
    "        return np.array(embeddings_dict[word])\n",
    "    else:\n",
    "        return np.mean(np.array(list(embeddings_dict.values())), axis=0)\n",
    "\n",
    "# Function to calculate string similarity using SequenceMatcher\n",
    "def string_similarity(word1, word2):\n",
    "    return SequenceMatcher(None, word1, word2).ratio()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cfc9db06-cee7-4116-9c9c-36bf6e726d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified function to get closest alias\n",
    "def get_closest_alias(name, names_list, embeddings_dict, threshold=0.7):\n",
    "    name_vector = get_word_vector(name, embeddings_dict)\n",
    "    best_match = None\n",
    "    highest_similarity = threshold\n",
    "    \n",
    "    for other_name in names_list:\n",
    "        if other_name == name:\n",
    "            continue\n",
    "        \n",
    "        if other_name.lower() in embeddings_dict:\n",
    "            other_vector = get_word_vector(other_name, embeddings_dict)\n",
    "            similarity = cosine_similarity([name_vector], [other_vector])[0][0]\n",
    "        else:\n",
    "            similarity = string_similarity(name.lower(), other_name.lower())\n",
    "        \n",
    "        if similarity > highest_similarity:\n",
    "            highest_similarity = similarity\n",
    "            best_match = other_name\n",
    "    \n",
    "    return best_match if best_match else name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "60442962-760d-4bc6-a31b-b5ca15a8532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('/Users/saiyarlagadda/Desktop/Neural-Network-Patient-Matching/my_project/alias_analysis/data/60k_records.csv')\n",
    "\n",
    "# Normalize names to lowercase\n",
    "df['name'] = df['name'].str.lower()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a643bd4-f187-43bb-853d-b301c0c3eb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique names and determine aliases\n",
    "unique_names = df['name'].unique()\n",
    "aliases = {name: get_closest_alias(name, unique_names, embeddings_dict) for name in unique_names}\n",
    "\n",
    "# Map aliases to the dataframe\n",
    "df['alias'] = df['name'].map(aliases)\n",
    "\n",
    "# # Save the updated dataframe\n",
    "# df.to_csv('60k_dataset_with_aliases.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5811d10-2541-4ec8-9d03-f91937f01163",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display the first few rows to verify\n",
    "print(df.head(10))\n",
    "\n",
    "# Aggregating primary alias counts\n",
    "primary_alias_counts = df['alias'].value_counts()\n",
    "print(primary_alias_counts.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5704535-fd23-4ad1-91ad-1a9eeebcabdd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Test June check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45e12204-c771-4608-bb60-fde1e1e19809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GloVe embeddings\n",
    "with open('/Users/saiyarlagadda/Desktop/Neural-Network-Patient-Matching/my_project/alias_analysis/pickle-dicts/glove-6B-100d.pkl', 'rb') as f:\n",
    "    embeddings_dict = pickle.load(f)\n",
    "\n",
    "# Function to get vector for a word, with fallback to average vector\n",
    "def get_word_vector(word, embeddings_dict):\n",
    "    word = word.lower()\n",
    "    if word in embeddings_dict:\n",
    "        return np.array(embeddings_dict[word])\n",
    "    else:\n",
    "        return np.mean(np.array(list(embeddings_dict.values())), axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f5d5865-36bc-472c-9acf-bbe7b64b795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate string similarity using SequenceMatcher\n",
    "def string_similarity(word1, word2):\n",
    "    return SequenceMatcher(None, word1, word2).ratio()\n",
    "\n",
    "# Modified function to get closest alias\n",
    "def get_closest_alias(name, original_patients, embeddings_dict, threshold=0.7):\n",
    "    name_vector = get_word_vector(name, embeddings_dict)\n",
    "    best_match = None\n",
    "    highest_similarity = threshold\n",
    "    \n",
    "    for patient in original_patients:\n",
    "        patient_name = patient.lower()\n",
    "        \n",
    "        if patient_name in embeddings_dict:\n",
    "            patient_vector = get_word_vector(patient_name, embeddings_dict)\n",
    "            similarity = cosine_similarity([name_vector], [patient_vector])[0][0]\n",
    "        else:\n",
    "            similarity = string_similarity(name.lower(), patient_name)\n",
    "        \n",
    "        if similarity > highest_similarity:\n",
    "            highest_similarity = similarity\n",
    "            best_match = patient\n",
    "    \n",
    "    return best_match if best_match else name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc0286c-102f-47a8-bbe0-a9b2c21a9f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('/Users/saiyarlagadda/Desktop/Neural-Network-Patient-Matching/my_project/alias_analysis/data/60k_records.csv')\n",
    "\n",
    "# Load the original 100 unique patients from the 'data' DataFrame\n",
    "original_patients = data['name'].str.lower().tolist()\n",
    "\n",
    "# Normalize names to lowercase in the 60k dataset\n",
    "df['name'] = df['name'].str.lower()\n",
    "\n",
    "# Get unique names and determine aliases based on original patients\n",
    "unique_names = df['name'].unique()\n",
    "aliases = {name: get_closest_alias(name, original_patients, embeddings_dict) for name in unique_names}\n",
    "\n",
    "# Map aliases to the dataframe\n",
    "df['alias'] = df['name'].map(aliases)\n",
    "\n",
    "# Save the updated dataframe\n",
    "# df.to_csv('/mnt/data/60k_dataset_with_aliases.csv', index=False)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(df.head(10))\n",
    "\n",
    "# Aggregating primary alias counts\n",
    "primary_alias_counts = df['alias'].value_counts()\n",
    "print(primary_alias_counts.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81d469a-f973-45e0-bb20-b1dc75f9666b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
